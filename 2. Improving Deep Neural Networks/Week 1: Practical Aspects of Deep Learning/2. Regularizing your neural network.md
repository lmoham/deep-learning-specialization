# Regularization

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.1.0.jpg" width="500"/>

* An implementation of L2 / L1 regularization for logistic regression
    * Sparsity of L1 apparently helps w/ compression of model for storage
* Can regularize bias term, but is unneeded because it's only 1 value

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.1.1.jpg" width="500"/>

* Frobenius norm (technically L2 norm) and how its calculated over the weights for all layers
* Is also known as "weight decay"

# Why regularization reduces overfitting?

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.2.0.jpg" width="500"/>

* If lambda were set to be really big, it would zero-out some of our weights, making our result dependent on the weights not zeroed out
* Thus, regularization reduces the impact a weight would otherwise have on our result

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.2.1.jpg" width="500"/>

* If we regularize the weights, z would therefore decrease. If z is lowered, then its range of outputs is limited for its function
    * So, in the tanh example, it would be limited to the linear portion of tanh
    * We essentially make our function work as a giant linear model, rather than a tanh one

# Dropout Regularization

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.3.0.jpg" width="500"/>

* Instead of reducing the impact of our weights, we straight up cull an entire node for a single example
    * You then do backprop on the culled network
* This act of randomized culling of weights happens for all examples

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.3.1.jpg" width="500"/>

* keep.prob = Probability that we keep a node in a hidden layer
* Generate a vector that is the same size as a hidden layer containing information about which node to remove (d3)
* a3 (3rd layer) * d3 = a3 w/ culled nodes
* To keep the values consistent, a3 is then scaled as follows when computing z: a3 / keep.prob
    * This is because dropout reduces the value of our activations by about 20%
* This is how to do inverted regularization and it helps w/ scaling at test time (?)
    * On average (for 50 nodes), 10 will be unactivated

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.3.2.jpg" width="500"/>

* Dropout is not applied because we don't want to randomize our outputs
* Can we? Sure, but it's computationally inefficient

# Understanding Dropout

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.4.0.jpg" width="500"/>

* Works kind of like shrinking your weights
* Goal: Make it so the network isn't too reliant on any particular feature
* For layers where we are afraid of overfitting (many nodes), we can reduce keep.prob
    * Likewise, if we have a smaller layer, we can increase keep.prob or even make it 1.0
* Dropout makes it hard to visualize our J over iterations
    * Solution: Train your model w/o dropout to see if the cost is reducing as it should, then implement dropout
    
# Other regularization models

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.5.0.jpg" width="500"/>

* Data augmentation helps w/ overfitting
    * Adding artificial data to the dataset (only modifications that keep the image realistic)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/2.5.1.jpg" width="500"/>

* Early stopping allows us to use the norm that gets decent loss after a single iteration plot
* Problem: Conflicts with orthogonalization
    * Orthogonalization: The concept of keeping a task within a specific goal
    * W/ early stopping, you now have to worry about optimizing loss function and regularization at the same time
* Using L2 normalization avoids this problem, but will require multiple iterations to optimize lambda
