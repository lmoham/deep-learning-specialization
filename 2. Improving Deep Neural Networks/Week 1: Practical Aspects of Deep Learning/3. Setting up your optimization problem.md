# Normalizing inputs

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.1.0.jpg" width="500"/>

* Normalizing training set: (x - mean) / stddev
    * Note that the test set must also be normalized in the exact way
    
<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.1.1.jpg" width="500"/>

* If unnormalized: gradient descent will take time to get to the global min
    * Extremely weird contour is the reason why GD will take time
* Normalized: Contour and its global min is clearly defined
    * Will take fewer iterations to converge

# Vanishing / Exploding gradients

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.2.0.jpg" width="500"/>

* Some assumptions for the explanation:
    * bias is set to 0
    * all weight matrices are slightly larger / smaller than identity matrix (barring the last one)
* If larger, our yhat and therefore our gradients will become exponentially larger
* If smaller, our yhat and gradients become exponentially smaller
* In both cases, training becomes harder

# Weight Initialization for Deep Networks

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.3.0.jpg" width="500"/>

* General formula: np.random.randn(shape) * np.sqrt(I)
    * Where "I" changes depending on initialization:
        * ReLU: 2 / n<sup>l-1</sup>
        * Xavier Initialization: 1 / n<sup>l-1</sup>

# Numerical approximation of gradients

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.4.0.jpg" width="500"/>

* We look at gradient + and - epsilon for gradient checking
    * The math shows that we get a more accurate look at how close our gradients are to the expected value
    * 0.0001 vs 0.03 difference w/ original gradient? The former gives us greater confidence

# Gradient checking

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.5.0.jpg" width="500"/>

* Establishing the problem:
    * Concatenate all weight and bias vectors into theta
    * Concatenate all derivative weight and bias vectors in d-theta
    * Is d-theta is gradient of theta?

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.5.1.jpg" width="500"/>

* For every i, compute the difference of 2 vectors theta, where theta[i] is added and subtracted by epsilon
* Compare resulting vector d-theta-approx with d-theta
    * If difference = 10<sup>-7</sup>: Our implementation seems to be correct
    * If difference = 10<sup>-3</sup>: Seriously consider looking for bugs or incorrections

# Gradient Checking Implementation Notes

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/3.6.0.jpg" width="500"/>

* Some tips on ensuring your gradient checking is done correctly
