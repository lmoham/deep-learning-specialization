# Train / Dev / Test sets

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.1.0.jpg" width="500"/>

* Won't get it right the first try, will require multiple iterations for better hyperparameters
* Knowledge from specific domains don't translate well across different domains
* The more you iterate, the better of an idea you get for the model you want to create

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.1.1.jpg" width="500"/>

* Dataset splits usually consist of 60/20/20
* However, if you have an extremely large dataset, you can get away with greedier percentages for the training set
    * Something like 98/1/1 could be considered acceptable

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.1.2.jpg" width="500"/>

* Make sure the train / dev / test sets comes from a similar distribution
* In certain situations, you can exclude the test set

# Bias / Variance

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.2.0.jpg" width="500"/>

* Remember bias and variance? Here's an example of how they look in a 2D distribution

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.2.1.jpg" width="500"/>

* In a higher-dimension dataset, it's harder to imagine, so we look at train / test error
* Whether your model is biased or varianced depends on the teo values relative to the bayes error
    * Bayes = optimal error value for a given task

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.2.2.jpg" width="500"/>

* What we can expect a high bias / high variance model to do (underfits in some ways, overfits in others)

# Basic Recipe for Machine Learning

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week1/1.3.0.jpg" width="500"/>

* An approach to fixing the high bias / variance problem
* There isn't really a tradeoff here (i.e. adding more data typically doesn't hurt the bias too much), which is part of why deep learning is so useful
