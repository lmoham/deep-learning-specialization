# Tuning process

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.0.jpg" width="500"/>

* What hyperparameters to focus on in order of importance: Red, Orange, Purple
    * This is subjective and may vary among other practitioners

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.1.jpg" width="500"/>

* Should try random values
* Especially if its unclear about which parameter is more useful to the problem

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.2.jpg" width="500"/>

* If you get the best results from some particular values, you can test within the region of those values

# Using an appropriate scale to pick hyperparameters

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.2.0.jpg" width="500"/>

* Here's one way you can pick your parameters
* Other parameters may require different approaches

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.2.1.jpg" width="500"/>

* Using a logarithmic scale to test values for alpha
* Makes more sense to divide your search into 4 different parts
    * Otherwise, majority of the values are going to come from the 0.1 - 1 range

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.2.2.jpg" width="500"/>

* Similar concept to the logarithmic scale
    * This is why we measure 1 - beta, instead of just beta by itself

# Hyperparameters tuning in practice: Pandas vs Caviar

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.3.0.jpg" width="500"/>

* Applications in a particular domain can potentially be used in other domains
* Important to re-evaluate parameters (the change in data and technology can play a factor in how well your current parameters work)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.3.1.jpg" width="500"/>

* Two different approaches to model training:
    * Panda: Carefully watch one model and fine-tune its parameters as the days go by
    * Caviar: Whip out a bunch of models then just pick the best one
