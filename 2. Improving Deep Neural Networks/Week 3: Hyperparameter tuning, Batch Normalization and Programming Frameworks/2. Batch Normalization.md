# Normalizing activations in a network

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.0.jpg" width="500"/>

* Normalizing our dataset makes our gradient descent more efficient; Can we do that for activations?
* The answer is yes and it is typically applied to z (i.e. before activations are applied)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.0.jpg" width="500"/>

* Normalization involves subtracting the mean then dividing by the variance
* However, if we were to use a function like sigmoid, we don't want to limit our values to a particular range
    * Gamma and beta allows us to control this
        * This way, we get the benefit of normalization, while also keeping our range of possible values realistic

# Fitting Batch Norm into a neural network

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/1.1.0.jpg" width="500"/>

* 
