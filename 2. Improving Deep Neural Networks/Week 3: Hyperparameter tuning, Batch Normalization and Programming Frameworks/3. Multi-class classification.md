# Softmax Regression

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.1.0.jpg" width="500"/>

* Binary classification - Our yhat is the probability that our input is positive (1 x 1 dimensions)
  * Multi-class - Probabilities that our input belongs to each possible class
    * (c x 1) dimensions, where c is the number of classes

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.1.1.jpg" width="500"/>

* Calculating softmax activations to get multiple probabilities
  * We get a vector of probabilities, rather than a single one, for each class

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.1.2.jpg" width="500"/>

* Softmax activation with no hidden layers
  * With hidden layers, we can calculate more complex boundaries between the classes

# Training a Softmax classifier

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.2.0.jpg" width="500"/>

* How softmax works
  * Hardmax straight up assigns a class to an input, rather than assigning a probability to a class

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.2.1.jpg" width="500"/>

* Our loss and cost functions with softmax activation

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week3/3.2.2.jpg" width="500"/>

* Implementing gradient descent for softmax activation
