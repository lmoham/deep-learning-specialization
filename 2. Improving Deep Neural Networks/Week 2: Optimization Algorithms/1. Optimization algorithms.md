# Mini-batch gradient descent

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.1.0.jpg" width="500"/>

* If we had a large number of training examples, then updating thru gradient descent will take a long time
    * Because we have to run thru the entire set
* Instead, we can use mini-batch gradient descent
    * This way, we can update gradients after going thru a portion of the data, not the entire thing

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.1.1.jpg" width="500"/>

* A coded implementation looks very similar to what we do normally when training our NN model
    * Difference is we train over batches of X, instead of X in its entirety

# Understanding mini-batch gradient descent

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.2.0.jpg" width="500"/>

* What you can expect to see when doing mini-batch gradient descent
    * Plot is not as smooth, but it does end up lowering in cost (after enough epochs)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.2.1.jpg" width="500"/>

* There is a difference in performance, based on batch size:
    * If size = m, then iterations take too long
        * note: size = m is called batch gradient descent (as in, we use the entire batch)
    * If size = 1, then vectorization speed is lost in the additional processing required for each entry
        * note: size = 1 is called stochastic gradient descent
    * Goal is to choose a value between 1 and m
        * This way, you get the benefit of vectorization, while also able to iterate quickly
        
<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.2.2.jpg" width="500"/>

* Batch is perfectly okay if m is small (around 2000 perhaps)
* Otherwise, you use a power-of-two-sized batch (64, 128, etc.)
    * This is because of the way computer memory is accessed
    * Ensure that this batch can fit in CPU / GPU memory

# Expotentially weighted averages

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.3.0.jpg" width="500"/>

* An example of how to compute the trends in temperature as the days go by

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.3.1.jpg" width="500"/>

* Generalizing the formula of expoentially weighted averages
    * β - Approximately how many days we want to average over
        * Larger the value, the smoother the graph, but the slower it adapts to current temps
        * Likewise, smaller values make noisier plots, but can adapt easier

# Understanding exponentially weighted averages

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.4.0.jpg" width="500"/>

* Formula for exponentially weighted averages and how the plot changes as β changes (red to green to yellow)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.4.1.jpg" width="500"/>

* How the function exponentially decays
* Notice how β gets exponentially larger as you go through previous thetas

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.4.2.jpg" width="500"/>

* Coded implementation of ewa

# Bias correction in exponentially weighted averages

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.5.0.jpg" width="500"/>

* When implementing ewa, the initial values will be slightly off
* Bias correction solves this problem, and has the added benefit of not affecting later examples

# Gradient descent with momentum

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.6.0.jpg" width="500"/>

* Momentum allows us to increase our horizontal movements, while limiting our vertical movements
    * Result: Plot in red 
    * If we were to increase learning rate, then we would get purple (increase horizontal, but we also get too much vertical)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.6.1.jpg" width="500"/>

* How to implement in code
* There are some implementations that exclude the (1  - β) term, but both should work just fine

# RMSprop

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.7.0.jpg" width="500"/>

* Same principle: Limit vertical movement, while maximizing horizantal movement
* RMS = root mean square
    * Take the square of derivative of W and B in their respective RMSprop variables (denoted as S)
    * Divide the derivative of the root of this variable
        * Add a small value epsilon to ensure numerical stability

# Adam optimization algorithm

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.8.0.jpg" width="500"/>

* It combines momentum with RMSprop
* Bias correction is applied to both momentum and RMSprop variables

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.8.1.jpg" width="500"/>

* Adam - Adaptive moment estimation
* Some recommended hyperparameters for the Adam optimizer

# Learning rate decay

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.9.0.jpg" width="500"/>

* With learning rate decay, we can rapidly move towards the global minimum like usual...
    * ...but once we're close enough, LR decreases, resulting in tighter oscillations around the minimum
    
<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.9.1.jpg" width="500"/>

* Formula for decaying learning rate on later iterations
* Later epochs result in smaller values of alpha

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.9.2.jpg" width="500"/>

* Other ways of decaying learning rate

# The problem of local optima

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.10.0.jpg" width="500"/>

* Local optima = zero gradient = cannot converge to global minimum
* Previously thought cost functions of NN look like those on the left
    * Actually, they form kind of a saddle (like the one on the right)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks/images/week2/1.10.1.jpg" width="500"/>

* So local optima realistically isn't a threat to our model..
    * ..plateaus are (gradients are near-zero)
    * This is where optimzation algorithms help us to reach the global minimum faster
