# Single number evaluation metric

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.1.0.jpg" width="500"/>

* How to measure performance? F1 score is an example (if you care about precision and recall)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.1.1.jpg" width="500"/>

* Metric for multi-class error could be taking the average of all errors

# Satisficing and Optimizing metric

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.2.0.jpg" width="500"/>

* Say we want to select based on some equation combining cost and running time
  * Kinda abitrary and artificial to do this (maybe even nonsensical?)
* Instead, by having an optimizing and satificing metric, we simplify the process of model selection
* For example:
  * Optimize: Accuracy
  * Satisfice: Running time below 100ms
    * Thus, based on these two metrics, we pick the classifier with the higher accuracy...
    * ...that is also below 100ms

# Train/dev/test distributions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.3.0.jpg" width="500"/>

* Need to ensure that the dev and test sets are from the same distribution
  * Otherwise, you are optimizing for the wrong target = time wasted

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.3.1.jpg" width="500"/>

* Example: Team was optimizing one problem...
  * ...but was then asked to see performance on a different problem
  * Oops! Time wasted.

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.3.2.jpg" width="500"/>

* Establishing our guideline

# Size of the dev and test sets

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.4.0.jpg" width="500"/>

* Difference between old and "new" way of splitting data:
  * We have so much more data available to us now
  * Thus, we can get away with using a smaller percentage for dev and test sets
    * Since there will still be thousands of examples for those sets
    * In addition, we get to strengthen our model with a larger training set

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.4.1.jpg" width="500"/>

* Need to ensure high accuracy? Perhaps consider using a larger test set
* Otherwise, you can get away with smaller test sets
  * There are cases where a test set is excluded completely (not recommended)

# When to change dev/test sets and metrics

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.5.0.jpg" width="500"/>

* Let's say an algorithm has a lesser error (i.e. correctly displays cats more often), but also displays inappropriate images
* Another algorithm has a higher error, but doesn't display these images
  * In this case, you are better off with the latter, since we want to avoid those in something like a cat app
  * Thus, it will be necessary to change your metric: High accuracy and doesn't display inappropriate material
  * One way this is done is by adding some weight to our error function that heavily punishes inappropriateness
    * Will require manually labeling those images within the dev and test sets

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.5.1.jpg" width="500"/>

* It can be kind of hard to recognize such an issue early on
* Make things easy: Establish your metric (target) and worry about aiming (is this what we want) separately
  * In other words: Work on your accuracy, then see if you are getting the functionality you want

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week1/2.5.2.jpg" width="500"/>

* Example: You use a lower error classifier, but turns out the "worse" one does better with users
  * A closer look reveals that the user images differ from the dev / test set
  * Indicative of a need to change your metric and / or dev / test sets
