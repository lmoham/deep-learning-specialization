# Training and testing on different distributions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.1.0.jpg" width="500"/>

* We have very little of the data we want to classify, but plenty that are different from our wanted dataset
  * Option 1 isn't recommended because we start to optimize the unwanted data (since it makes up most of the dev/test distribution)
  * Option 2 is best. Dev/test is made up of ONLY our wanted data, while training consists of the mixture of both

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.1.1.jpg" width="500"/>

* Speech recognition example:
  * Dev/test set consists of ONLY speech activated rearview mirror
  * Training set comes from either an entirely different distribution or a mix of the two...
    * ...where the unmatching distribution consists of the majority

# Bias and Variance with mismatched data distributions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.2.0.jpg" width="500"/>

* How to correctly identify the problem when using mixmatched data distributions (between training and dev/test)
  * Large error difference between training and training-dev = variance problem
  * Large error difference between training-dev and dev = data mismatch problem

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.2.1.jpg" width="500"/>

* The various types of problems to face and where they occur
* There may be situations where the dev/test set error is lower than the training-dev error

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.2.2.jpg" width="500"/>

* Column = data type, row = error type
* Can draw it out this way to maybe gain additional insights into our results
  * Example: Car speech (right) is actually harder for humans than  general speech
  * We still managed to get our dev/test error lower than the training error
  * Avoidable bias is therefore something that needs to be dealt with

# Addressing data mismatch

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.3.0.jpg" width="500"/>

* Try to manually find the difference between training and dev/test sets
* Example: general speech vs car speech
  * Lots of car noise and street numbers in the latter (the driver is trying to find directions as the car is moving)
  * We can try to simulate this effect by adding car noise to our training set

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.3.1.jpg" width="500"/>

* Process of synthesizing a new clip that simulates speech in a car
* One problem: Suppose we only have 1 hour of car noise and we use it for all training examples
  * Our model may overfit to this specific car noise (computers can see noise patterns much more finely than humans)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/2.3.2.jpg" width="500"/>

* Car recognition can suffer from the same problem (in synthesizing)
* If you don't generate enough unique examples, you will start to overfit on the type of synthesis you used
