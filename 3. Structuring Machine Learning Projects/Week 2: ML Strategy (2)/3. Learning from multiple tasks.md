# Transfer learning

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/3.1.0.jpg" width="500"/>

* Transfer learning is using a model trained on one task for an entirely new task
  * Pre-trained model - Model that was trained for one task
  * Fine-tuned model - Model that was retrained for a new task
* Case scenarios:
  * Not many data for radiology diagnosis, but an image recognition model was trained on millions of data
    * The insights obtained in such a model could have its uses in radiology
  * Speech recognition model trained on 10h of general data and you have 50h of wakeword data
    * You are probably better off creating a new model
    * REALLY hard to generalize when trained off of such little data, so it makes no sense to transfer learn such a model
* It works by replacing the last node w/ a new one w/ new weights (W and b)
  * Sometimes even more layers are added to the already existing model
  * You can then opt to train just the newly added layers or even the entire model altogether

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/3.1.1.jpg" width="500"/>

* Some guidelines to decide whether transfer learning should be done
* Low level features = curves, lines, dots (in image recognition)

# Multi-task learning

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/3.2.0.jpg" width="500"/>

* Example: Identifying the prescence of multiple objects
  * So our training set would be of 4 x m dimensions, where 4 is the number of objects to detect

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/3.2.1.jpg" width="500"/>

* The end layer now contains 4 nodes because we are trying to identify the presence of 4 different objects
* Loss function is also described
  * Log loss is used instead of softmax (w/ softmax, you assign ONE label based on the multiple probabilities obtained)
* Works even if some values are missing (for example, 2 out of 4 values are defined as 0 or 1, the rest empty)
  * Loss function will simply sum the values that exist and omit the ones that don't

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/3.%20Structuring%20Machine%20Learning%20Projects/images/week2/3.2.2.jpg" width="500"/>

* Some guidelines to decide whether multi-task learning makes sense
* If you had 1000 data for each task and subsequent tasks have some relation to earlier data (despite the task differences), this can help in later tasks, IF you have plenty of data for the previous tasks
