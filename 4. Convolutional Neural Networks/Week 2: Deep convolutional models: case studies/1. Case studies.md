# Why look at case studies?

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.1.0.jpg" width="500"/>

* Networks we will be looking at
* We can get a better understanding of the inner workings of NN by looking at some classic network implementations

# Classic Networks

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.2.0.jpg" width="500"/>

* Overview of the LeNet architecture
* Details in red = obsoleted features that are in the paper
* Patterns that are common even today: 
  * Decreasing size, but increasing nunber of channels
  * Conv -> pool -> conv -> pool -> FC -> FC -> output

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.2.1.jpg" width="500"/>

* AlexNet architecture
* Similar to LeNet, but w/ much more parameters and uses ReLU instead of sigmoid/tanh

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.2.2.jpg" width="500"/>

* VGG-16 implementation
* Uses consecutive convolution layers, conv and pooling parameters remain constant

# ResNets

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.3.0.jpg" width="500"/>

* Residual blocks add inputs from an earlier layer to a later one

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.3.1.jpg" width="500"/>

* The vanishing/exploding gradient becomes a problem if the number of layers is too much
  * This becomes evident in the decrease of accuracy on the left plot
  * Problem is solved w/ the use of residual blocks (in this example, they happen every 2 layers for a total of 5 times)

# Why ResNets Work

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.4.0.jpg" width="500"/>

* Identity functions are easy to learn
  * Important because in deeper layers, learning the right parameters becomes very difficult
* Z<sup>[l+2]</sup> needs to be the same dimension as a<sup>[l]</sup>
  * Achieved by using "same" padding or multiplying a<sup>[l]</sup> w/ W<sub>s</sub> (which can also have its own parameters, if you choose)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.4.1.jpg" width="500"/>

* Difference between a "plain network" and a ResNet
* Consistent "same" convolutions make the residual block calculations possible

# Networks in Networks and 1x1 Convolutions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.5.0.jpg" width="500"/>

* Multiply a single slice (same height and width across 32 channels) w/ our filter, then apply a nonlinearity
  * The resulting single number is then placed in its respective location
  * If there are additional filters, then this single slice will be multiplied with each one

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.5.1.jpg" width="500"/>

* 1x1 convolutions allow us to reduce the number of channels
  * Similar to how pooling layers reduce the dimensions of our input
* Even if you didn't want to reduce # of channels, it can still serve as a nonlinearity to help learn the data better

# Inception Network Motivation

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.6.0.jpg" width="500"/>

* An inception network makes use of convolutions of multiple sizes
  * The network will then learn w/e combinations of filter sizes it wants
  * Useful if unsure about what kind of convolutions we want to use (or if we want to use a pooling layer instead)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.6.1.jpg" width="500"/>

* Computationally expensive to run a 5x5 filter, even if running on modern hardware

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.6.2.jpg" width="500"/>

* We can instead use a 1x1 convolution layer to reduce the number of channels from 192 to 16
* THEN we can apply the 5x5 filter
  * End result gets us the desired dimension, but number of computations is reduced from 120M to 12.4M
  * The 1x1 conv layer is also known as a bottleneck layer and doesn't impact model performance w/ proper implementation

# Inception Network

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.7.0.jpg" width="500"/>

* An exampmle of what a portion of an inception module looks like
* MaxPooling layer uses "same" padding to ensure same dimensions
  * 1x1 conv layer is used to reduce # of pooling channels (192 is a bit excessive)
* Output is 28x28x256 (concatenate all of the outputs from all the layers)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/1.7.1.jpg" width="500"/>

* Contains a bunch of inception modules
* Contains a few side branches that calculate softmax from a hidden layer
  * Used to ensure that features we are creating are helping with our prediction task
