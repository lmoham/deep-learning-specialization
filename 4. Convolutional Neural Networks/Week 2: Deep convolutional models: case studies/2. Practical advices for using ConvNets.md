# Using Open-Source Implementation

* Open-source code allows us to make use of powerful networks, without investing the time and hardware needed to train them
  * Transfer learning: Using a pre-trained network instead of creating your own

# Transfer Learning

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.1.0.jpg" width="500"/>

* We take a pre-trained model and change the last layer to output something we want to predict
  * trainableParameters = 0 and freeze = 1 helps us to freeze earlier layers, so that only our softmax layer is trained and updated
  * You can choose to freeze lesser layers if you have a somewhat large dataset - Then you train the last few layers + softmax layer
  * Can retrain entire network if you have access to ridiculously large amounts of data and computational power

# Data Augmentation

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.2.0.jpg" width="500"/>

* Common augmentation methods
* Methods in the curly brace aren't commonly used

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.2.1.jpg" width="500"/>

* Color shifting: Adding and subtracting to all three color channels to generate different colored images
  * PCA color augmentation is another color shifting technique that is dependant on the ratio of each color to each other

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.2.2.jpg" width="500"/>

* How distortions can be done in practice
  * You can perform the distortions and training in parallel, reducing the amount of time required per iteration

# State of Computer Vision

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.3.0.jpg" width="500"/>

* Lots of data = Less engineering required w/ simpler algorithms
* Less data = More hand-engineered and complex features/architectures and can use transfer learning

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.3.1.jpg" width="500"/>

* Some tips on  how to do well on benchmarks/winning competitions
  * Typically aren't used in production because they slow down running time

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week2/2.3.2.jpg" width="500"/>

* Brief refresher on transfer learning
