# Computer Vision

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.1.0.jpg" width="500"/>

* Some examples of computer vision applications

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.1.1.jpg" width="500"/>

* Problem: Images have so many parameters and the bigger the image, the more parameters you need
  * This is because we read an image as the product of its size (pixel length x pixel width) and color channels (3: R,G,B)
    * For example, a 1000px by 1000px image would require 3 million parameters (1000 x 1000 x 3)

# Edge Detection Example

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.0.jpg" width="500"/>

* In a CNN, the deeper you go, the more complex the features detected (and vice versa)
* In the first layer, the CNN tries to detect edges within the images
  * There are two types of edges: vertical and horizontal

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.1.jpg" width="500"/>

* Say we have a grey-scale 6x6 image (so 6x6x1 parameters) and we want to detect edges
* We take that image and convolve it with a filter (also known as a kernel)
   1. Start from the upper-left corner, then take the sum of the element-wise product of the filter and its equivalent in the image
      * This sum is used to create the first pixel of our new image
   2. Then, shift one to the right and repeat. Place this sum next to our previous sum
   3. Once we can no longer shift right, we shift one down, starting from our initial position. We do the same for our new image
   4. This process of shifting and adding products to form a new image repeats until we covered all pixels of the original image
* Image convolve with a filter produces a new image

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.2.jpg" width="500"/>

* Why edge detection works
* You can think of the filter as looking for an area where there are light pixels on the left (1) and dark pixels on the right (-1)
  * An edge is essentially an area that divides two distinct colors (hence why the center contains all zeros)
  * The example is exaggerated because our image is so small, larger images will appear more realistic

# More Edge Detection

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.0.jpg" width="500"/>

* Light to dark transition vs dark to light

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.1.jpg" width="500"/>

* Vertical vs horizantal edge
  * Both are looking for bright colors on one side and darker colors on the other

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.2.jpg" width="500"/>

* There are other types of filters like the Sobel or Scharr filters
* But, we can also just treat the filter as additional parameters for the neural network to train
  * This way, we don't have to hard code a specific filter - the model will figure out what is the optimal filter to use
  * In addition, this allows the network to learn more complex edges, rather than just strictly vertical and horizontal

# Padding

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.4.0.jpg" width="500"/>

* Formula for the dimensions of new image: (n - f + 1) by (n - f + 1)
  * Where n x n is the original image and f x f are the filter dimensions
* The problems with this approach:
  * Shrinking output: our new image will always be smaller than the original
  * Edges: Information from corner edge is lost (we only use it ONCE)
* Padding allows us to solve these problems
  * Example: If we add a padding of 1, then we get to see more of the corner edge in our calculations
  * Formula for dimensions of new image (w/ padding): (n + 2p - f + 1) by (n + 2p - f + 1)
    * Where p is how much we pad by
  * The resulting image retains the size of our original image of 6 x 6 w/ a padding of 1

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.4.1.jpg" width="500"/>

* Formulas for how to deduce resulting image from convolution
* Valid convolutions: no padding 
* Same convolutions: adding enough padding so that the output size equals the input
  * Formula: p = (f - 1) / 2
* Filter dimensions are typically odd
  * Odd dimensions contain a center pixel
  * If dimensions were even, asymmetric padding would be required

# Strided Convolutions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.0.jpg" width="500"/>

* Stride measures how big of a jump do we make (in the previous example we did strides of 1)
* Describes formula for finding the resulting dimensions w/ strides
  * If formula doesn't return a whole number, you round down
  * Convolutions that go beyond the image dimensions (+ w/e padding is used) are not calculated at all

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.1.jpg" width="500"/>

* Convolution summary w/ a general formula that applies to all convolution applications

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.2.jpg" width="500"/>

* TECHINICALLY we are performing cross-correlation
  * Convolution involves mirroring our filter (aka kernel) both horizontally and vertically
  * Certain applications make use of this, probably
  * Just something interesting to know, doesn't affect anything we do w/ CNNs

# Convolutions Over Volume

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.6.0.jpg" width="500"/>

* Previously we were working with 1-channel images
* 3-channel images follows the exact same process
  * Need to ensure that the image and filter share the same number of channels
  * Can modify each channel separately if looking for color-specific things in the image
* Sometimes the 3-channel filter will be represented as a cube (to denote a 3D filter)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.6.1.jpg" width="500"/>

* You can use additional filters to find additional features (one 3x3x3 filter for horizontal edges and another for vertical ones)
* The resulting images are then stacked ontop of each other
  * If we obtain 4x4 on one filter, then if we had two, our resulting image would be 4x4x2 (and so on)

# One Layer of a Convolutional Network

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.7.0.jpg" width="500"/>

* Some comparisons to make:
  * a<sup>[0]</sup> = x = our input image
  * filters = our weights
  * Z = input image * weights + bias
  * a<sup>[1]</sup> = g(Z), where g is a nonlinearity (like ReLU, for example)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.7.1.jpg" width="500"/>

* How to calculate the number of parameters in a layer given the number of filters used
* Note that it is not affected by the size of our input image (so, doesn't matter if it's big or small)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.7.2.jpg" width="500"/>

* Now introducing a bunch of notation

# Simple Convolutional Network Example

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.8.0.jpg" width="500"/>

* What a simple ConvNet could look like
* Some trends to notice:
  * The size of our image gradually decreases
  * The number of filters used increase
    * That being said, probably not true for all ConvNets (?)
* For our last layer, we flatten the image (unroll everything into a single-column vector), then calculate probability

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.8.1.jpg" width="500"/>

* Other types of layers you will find in ConvNets (they use more than just convolution layers)

# Pooling Layers

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.9.0.jpg" width="500"/>

* Similar to convolutions, pooling layers have a size and a stride
  * For the example above, the "filter" is 2x2 with a stride of 2
* However, unlike convolutions, there are no parameters, meaning gradient descent won't change anything
* Pooling layers are used to squeeze information from a number of pixels into just one
  * This way, we reduce computational complexity, while also keeping the most important features of our image
    * Example: Drawing a house with a thick brush vs a small brush  
      * Lots of unnecessary marks with the larger one
      * Small brush doesn't, but can still be interpreted as a house - This is our pooling layer

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.9.1.jpg" width="500"/>

* An example of max pooling
* If there are more channels, simply apply it to each channel
* The formula used to find the dimensions of the resulting image in convolutions also works here

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.9.2.jpg" width="500"/>

* Average pooling: Instead of taking the max, you take the average
* Used much less often than max pooling, can have its uses in later layers

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.9.3.jpg" width="500"/>

* Summary of pooling and all its notation
* No parameters to learn - One-time setup is all that's required
* Typically doesn't involve padding, unless in specific scenarios, which haven't been detailed yet

# CNN Example

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.10.0.jpg" width="500"/>

* What a CNN looks like for handwritten number recognition
  * As you go deeper, the size of the image decreases and the number of filters increase
  * A single layer consists of both a Conv and Pooling layer - Pooling doesn't have any parameters, so this is why
  * Structure: Conv -> Pool -> Conv -> Pool -> FC -> FC -> FC -> Softmax (want to assign ONE label)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.10.1.jpg" width="500"/>

* A more concrete look at our example CNN
  * Some of these calculations are wrong, but the main idea still holds
  * As expected, activation size continues to decrease (because our image gets smaller)
  * Most of our parameters belong in the FC layers, since they are fully-connected

# Why Convolutions?

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.11.0.jpg" width="500"/>

* Thanks to filters, in the example above, we use only 156 parameters, rather than 14 million
* Without this convoluting, the number of parameters we need would get extremely ridiculous at large image sizes

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.11.1.jpg" width="500"/>

* Some additional benefits
* We are generating a sparse model which means we get a concise interpretation of our image
  * In other words, we leave out the unnecessary details

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.11.2.jpg" width="500"/>

* Putting it all together
* The weights and biases associated with our filter in layer 1 and the FC layer in layer 2 can then be updated through some optimizer
