# Computer Vision

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.1.0.jpg" width="500"/>

* Some examples of computer vision applications

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.1.1.jpg" width="500"/>

* Problem: Images have so many parameters and the bigger the image, the more parameters you need
  * This is because we read an image as the product of its size (pixel length x pixel width) and color channels (3: R,G,B)
    * For example, a 1000px by 1000px image would require 3 million parameters (1000 x 1000 x 3)

# Edge Detection Example

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.0.jpg" width="500"/>

* In a CNN, the deeper you go, the more complex the features detected (and vice versa)
* In the first layer, the CNN tries to detect edges within the images
  * There are two types of edges: vertical and horizontal

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.1.jpg" width="500"/>

* Say we have a grey-scale 6x6 image (so 6x6x1 parameters) and we want to detect edges
* We take that image and convolve it with a filter (also known as a kernel)
   1. Start from the upper-left corner, then take the sum of the element-wise product of the filter and its equivalent in the image
      * This sum is used to create the first pixel of our new image
   2. Then, shift one to the right and repeat. Place this sum next to our previous sum
   3. Once we can no longer shift right, we shift one down, starting from our initial position. We do the same for our new image
   4. This process of shifting and adding products to form a new image repeats until we covered all pixels of the original image
* Image convolve with a filter produces a new image

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.2.2.jpg" width="500"/>

* Why edge detection works
* You can think of the filter as looking for an area where there are light pixels on the left (1) and dark pixels on the right (-1)
  * An edge is essentially an area that divides two distinct colors (hence why the center contains all zeros)
  * The example is exaggerated because our image is so small, larger images will appear more realistic

# More Edge Detection

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.0.jpg" width="500"/>

* Light to dark transition vs dark to light

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.1.jpg" width="500"/>

* Vertical vs horizantal edge
  * Both are looking for bright colors on one side and darker colors on the other

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.3.2.jpg" width="500"/>

* There are other types of filters like the Sobel or Scharr filters
* But, we can also just treat the filter as additional parameters for the neural network to train
  * This way, we don't have to hard code a specific filter - the model will figure out what is the optimal filter to use
  * In addition, this allows the network to learn more complex edges, rather than just strictly vertical and horizontal

# Padding

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.4.0.jpg" width="500"/>

* Formula for the dimensions of new image: (n - f + 1) by (n - f + 1)
  * Where n x n is the original image and f x f are the filter dimensions
* The problems with this approach:
  * Shrinking output: our new image will always be smaller than the original
  * Edges: Information from corner edge is lost (we only use it ONCE)
* Padding allows us to solve these problems
  * Example: If we add a padding of 1, then we get to see more of the corner edge in our calculations
  * Formula for dimensions of new image (w/ padding): (n + 2p - f + 1) by (n + 2p - f + 1)
    * Where p is how much we pad by
  * The resulting image retains the size of our original image of 6 x 6 w/ a padding of 1

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.4.1.jpg" width="500"/>

* Formulas for how to deduce resulting image from convolution
* Valid convolutions: no padding 
* Same convolutions: adding enough padding so that the output size equals the input
  * Formula: p = (f - 1) / 2
* Filter dimensions are typically odd
  * Odd dimensions contain a center pixel
  * If dimensions were even, asymmetric padding would be required

# Strided Convolutions

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.0.jpg" width="500"/>

* Stride measures how big of a jump do we make (in the previous example we did strides of 1)
* Describes formula for finding the resulting dimensions w/ strides
  * If formula doesn't return a whole number, you round down
  * Convolutions that go beyond the image dimensions (+ w/e padding is used) are not calculated at all

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.1.jpg" width="500"/>

* Convolution summary w/ a general formula that applies to all convolution applications

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.5.2.jpg" width="500"/>

* TECHINICALLY we are performing cross-correlation
  * Convolution involves mirroring our filter (aka kernel) both horizontally and vertically
  * Certain applications make use of this, probably
  * Just something interesting to know, doesn't affect anything we do w/ CNNs

# Convolutions Over Volume

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.6.0.jpg" width="500"/>

* Previously we were working with 1-channel images
* 3-channel images follows the exact same process
  * Need to ensure that the image and filter share the same number of channels
  * Can modify each channel separately if looking for color-specific things in the image
* Sometimes the 3-channel filter will be represented as a cube (to denote a 3D filter)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/4.%20Convolutional%20Neural%20Networks/images/week1/1.6.1.jpg" width="500"/>

* You can use additional filters to find additional features (one 3x3x3 filter for horizontal edges and another for vertical ones)
* The resulting images are then stacked ontop of each other
  * If we obtain 4x4 on one filter, then if we had two, our resulting image would be 4x4x2 (and so on)
