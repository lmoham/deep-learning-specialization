# Vectorization

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6vec.jpg width="500">

* Instead of using for-loops, use vectors to do the machine learning tasks
* GPUs are specially built to efficiently handle these tasks
* You can even see this by programming a time variable before and after a for-loop implementation and a vectorized one

# More Vectorization examples

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6morvec.jpg width="500">

* For-loops become increasingly slow the more loops you use

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6morvecdos.jpg width="500">

* Numpy is a python library that makes it possible to do a lot of this efficient computation
* If you need to do some kind of operation on a vector, chances are you're going to be using numpy

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6lrdvec.jpg width="500">

* This is the previously coded version of the derivatives in LR
* Picture shows how to vectorize some part of the code...
    * ...But how about the for loop at the top?
    
# Vectorizing Logistic Regression

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6lrvec.jpg width="500">

* Can compute Z (which is wTx + b) over the entire dataset with 1 line of code
* How to vectorize A (sigmoid Z)?

# Vectorizing Logistic Regression's Gradient Output

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6veclr.jpg width="500">

* Outlined in red is the highly convenient, one-code-line, vectorized form of computing parameter derivatives

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6veclrdos.jpg width="500">

* How to take our previous chunk of code and vectorize it
* Note that there is still a for loop required for actually iterating gradient descent multiple times
    * As it is right now, it is just one iteration
    
# Broadcasting in Python

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6broad.jpg width="500">

* Axis = 0 means to sum vertically
* Broadcasting - Each value of our nutrition matrix is divided by its respective calorie value, w/o needing to explicitly make that command

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6broadg.jpg width="500">

* If a matrix is of different dimensions, more will be added to make it calculable (on a per-element basis)
    * This also applies to single numbers - it will be duplicated to allow calculations across all elements

# A note on python/numpy vectors

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/6pyvec.jpg width="500">

* Need to be explicit in creating vector dimensions
* Otherwise, you get something that isn't actaully a vector
    * Taking the transpose will yield strange result
    * Multiplying will give you an inner product (as opposed to outer, which is what we want)
