# Binary Classification

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4bc.jpg width="500">

* Establishing the problem:
  * Let's say we want to identify whether an image contains a cat.
  * Each image will be 64 x 64 pixels
    * These pixels are represented by three 64 x 64 matrices, showing the degree of red, blue, and green of a particular pixel
    * These will be unrolled onto a single feature vector of 12,288 features (64x64x3)
  * Output - 1 indicates a cat is present, and vice versa

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4n.jpg width="500">

* Some notation:
  * x will have nx dimensions (representing the total number of pixels for an image)
  * X will have m by nx dimensions (where m = number of samples in dataset)
  * Y will have 1 by m dimensions 
    * Thus, X and Y both represent the dataset in columns

# Logistic Regression

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4n.jpg width="500">

* How it works:
  * We want to find y hat, which is the probability that y = 1 for some input
  * For our parameters, they are x (input), w (weights w/ nx dimensions), and b (a real number)
  * Our function for y hat can be written as a linear regression function 
  * But we don't want that b/c our values should be between 0 and 1
    * Linear regression function can give us large positive values and negative values (makes no sense)
    * Thus, we slap a sigmoid function in front of it. Now we will guarantee our values fall between 0 and 1
    * W/ sigmoid, if we have large value for the inside component, it keeps it closer to 1. 
    * Likewise, if we have small value, it keeps it closer to 0.

# Logistic Regression Cost Function

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4n.jpg width="500">

* Some more notation:
  * Remember, y hat is the probability that our output is 1 for any given input.
    * We want this to be as close to y as possible. The closer it is, the more accurate our predictions
  * *Loss function* - What we use to measure the error of a single example
    * We do NOT take mean squared error because, it makes the optimization problem non-convex (hard to find global minimum)
    * The smaller our loss, the better. Thus, y hat should be as small as possible. This way, we get as close to the expected value as possible.
  * *Cost function* - Averages the loss function over the entire dataset
