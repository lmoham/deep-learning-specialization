# Binary Classification

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4bc.jpg width="500">

* Establishing the problem:
  * Let's say we want to identify whether an image contains a cat
  * Each image will be 64 x 64 pixels
    * These pixels are represented by three 64 x 64 matrices, showing the degree of red, blue, and green of a particular pixel
    * These will be unrolled onto a single feature vector of 12,288 features (64x64x3)
  * Output - 1 indicates a cat is present, and vice versa

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4n.jpg width="500">

* Some notation:
  * x will have nx dimensions (representing the total number of pixels for an image)
  * X will have m by nx dimensions (where m = number of samples in dataset)
  * Y will have 1 by m dimensions 
    * Thus, X and Y both represent the dataset in columns

# Logistic Regression

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4lr.jpg width="500">

* How it works:
  * We want to find y hat, which is the probability that y = 1 for some input
  * For our parameters, they are x (input), w (weights w/ nx dimensions), and b (a real number)
  * Our function for y hat can be written as a linear regression function 
  * But we don't want that b/c our values should be between 0 and 1
    * Linear regression function can give us large positive values and negative values (makes no sense)
    * Thus, we slap a sigmoid function in front of it. Now we will guarantee our values fall between 0 and 1
    * W/ sigmoid, if we have large value for the inside component, it keeps it closer to 1
    * Likewise, if we have small value, it keeps it closer to 0

# Logistic Regression Cost Function

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4lrcf.jpg width="500">

* Some more notation:
  * Remember, y hat is the probability that our output is 1 for any given input
    * We want this to be as close to y as possible. The closer it is, the more accurate our predictions
  * *Loss function* - Measures how well we are doing on a single example
    * We do NOT take mean squared error because, it makes the optimization problem non-convex (hard to find global minimum)
    * The smaller our loss, the better. Thus, y hat should be as small as possible. This way, we get as close to the expected value as possible
  * *Cost function* - Averages the loss function over the entire dataset
    * It also measures how well our parameters w and b are doing on the entire dataset

# Gradient Descent

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4gd.jpg width="500">

* How it works:
  * With gradient descent, we try to find the parameters w and b that minimize the cost function
  * Doesn't matter where we intialize, since the function is convex (we are not using MSE)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4gddos.jpg width="500">

* How to update parameters:
  1. Take the derivative of the cost function with respect to both parameters w and b
  2. Subtract the original value of the parameter with the product of the learning rate alpha and the derivative
  3. Repeat until convergence at global minimum
* Graph = visual for what it looks like

# Derivatives

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4deri.jpg width="500">

* Derivatives are just finding the slope at a point
  * If you were to go an infinitesimally small distance forward or backward from a point, you will move up by a factor of the slope
  * Slope is just height divided by width

# More Derivative Examples

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4mderidos.jpg width="500">

* More examples of derivatives
* Slope varies on different points of the graph 

# Computation Graph

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4compgr.jpg width="500">

* Picture shows what happens in forward-propagation
  * Output is calculated
* Backward-propagation allows us to calculate the derivative
  * Remember, we need this to update our parameters 
  
# Derivatives with a Computation Graph

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/4compderidos.jpg width="500">

* Talks about chain rule
* We can find the derivative of the previous inputs of a node by finding one for that node
  * Example - By finding dJ/dV, we can find dJ/dU, by multiplying dJ/dV w/ dV/du
    * This process of finding derivatives happens until we can calculate the derivatives of all inputs

# Logistic Regression Gradient Descent

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/5lrre.jpg width="500">

* Just a quick recap of what we're doing
* Note that z changes based on the number of inputs (which also affects number of weights)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/5lrderi.jpg width="500">

* Calculating all possible derivatives at each node
* Once we've got our derivatives, we can update the parameters w1, w2, and b
* This is what happens in ONE training example

# Gradient Descent on m Examples

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/5lrmex.jpg width="500">

* Calculating cost - Average of loss over entire dataset
  * Calculating derivative of cost wrt. w1 - Average of derivative of loss wrt. w1
  * Think back to finding derivatives for our 1 example - We now apply this to ALL examples
  
<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/5lrmexdos.jpg width="500">

* Coded implementation:
  1. Create a for loop to go through every training example
  2. Accumulate J (remember, it is the average loss for the ENTIRE dataset)
  3. Accumulate the derivatives for inputs w1, w2, b
     * They are applied to ALL training examples, while z is unique to one example
     * This is why derivative of z does NOT accumulate
  4. Divide each accumulation with m
  5. Update parameters
* Weakness - For loops are extremely inefficient
  * The bigger the dataset, the slower our NN is going to perform
  * Solution? Use vectorization!
