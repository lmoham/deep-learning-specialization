![](https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/1title.jpg?raw=true)

# What is a neural network?

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/1hpp.jpg" width="500"/>

* Example - Predicting house prices w/ a function of house size:
  * A neural network will consist of a neuron which takes in size and outputs price
  * You can stack these neurons for bigger networks
  * Therefore, a neural network = Using neurons to take an input and produce an output
  * ReLU (Rectified linear unit) is used where negative numbers don't make sense (no such thing as negative prices, for example)

<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/1hppnn.jpg" width="500"/>

* What an example NN looks like:
  * The output of each neuron is an additional feature
  * We only supply the input and output features; The neuron features are determined on their own
  * What a neuron does is take the max of zero (im assuming this means positive part of zero)
  
<img src="https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/1hppnn_formalize.jpg" width="500"/>

* In practice:
  * We call neurons "hidden units"
  * Each hidden unit will take EACH input
     * *Densely connected* - Describes layers that are fully connected between each other
  * We don't explicitly say, "Hey, these two features are related to family size". We give the hidden unit each input and say, "What do you think?"
  * If enough examples are provided, NNs are REALLY good at figuring out how to map *x* to *y*.
