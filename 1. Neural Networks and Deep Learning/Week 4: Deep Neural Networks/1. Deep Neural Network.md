# Deep L-layer neural network

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.1.jpg width="500">

* Shallow = not many hidden layers, Deep = multiple hidden layers

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.2.jpg width="500">

* Going over some notation 

# Forward Propagation in a Deep Network

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.3.jpg width="500">

* Forward propagation represented in the notation described previously
* lower case = single training example, capitalized = entire datset (or at least the entire batch)

# Getting your matrix dimensions right

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.4.jpg width="500">

* Thought process behind how to determine correct number of features (non-vectorized)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.4.2.jpg width="500">

* Vectorized: Instead of a single example, we apply it over all examples (denoted by m)

# Why deep representations?

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.5.jpg width="500">

* The further you go into the NN, the more complex the features that are being detected from the dataset
    * Image and audio example in the video
    * For facial recognition, first hidden layer can only detect edges
        * 2nd layer can find more than edges; It can detect some features of the face

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.5.2.jpg width="500">

* Some more intuition on the effectiveness of deep learning
* Creating a simple network to represent the xor function would take waaaay too many hidden units (exponentailly increases)
    * More layers = no need to create a bunch of units = easier implementation

# Building blocks of deep neural networks

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.6.jpg width="500">

* Basic principle: For any layer, input something, get corresponding activations
    * But you need the weights and bias associated with a particular layer

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.6.2.jpg width="500">

* A more complete diagram of forward and backprop
* Cache: Storing variables that we want to reuse later for backprop

# Forward and Backward Propagation

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.7.jpg width="500">

* We propagate forward until we get our output y hat, along with caches of z

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.7.2.jpg width="500">

* We start with derivative of y hat to get the derivatives of the output of each layer, along with caches of weight derivatives

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.7.3.jpg width="500">

* Recap of what our NN is doing. 
* Vectorizng backprop involves getting the derivative of the loss wrt y hat over all training examples

# Parameters vs Hyperparameters

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.8.jpg width="500">

* Parameters: Features that the NN is trying to learn (weights, bias)
* Hyparameters: Features that we can control (learning rate, number of iterations, etc.)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.8.2.jpg width="500">

* Deep learning is an iterative process
* Finding the best hyperparameters will typically take multiple iterations
    * You can develop a general idea of what hyperparameters work best with what application / domain, given enough experience

# What does this have to do with the brain? ðŸ¤”

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week4/1.9.jpg width="500">

* "It's like the human brain" - We take in something (input) and get something out of it (output)
* The reality is that the neuron is very complex in nature and how it works remains a mystery to us
* Reality: Deep learning is very good at learning very flexible / complex functions to map an input to an output
