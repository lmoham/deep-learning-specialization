# Neural Networks Overview

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.1.jpg width="500">

* Quick review of our neural network and how to interpet it

# Neural Network Representation

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.2.jpg width="500">

* "a" stands for activations, so a<sup>[0]</sup> = activations in the respective layer
* When describing number of layers, it is convention to ignore the input layer
* "w" (our weights) has dimensions (number of nodes in current layer, number of inputs from previous layer)
    * Therefore, in our example here, it has (4,3) dimensions

# Computing a Neural Network's Output

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.3.jpg width="500">

* How calculations work in our neural network
* W = vector of weights W for a particular layer

# Vectorizing across multiple examples

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.4.jpg width="500">
* What our code should be doing
* Each column corresponds to a sample and each row corresponds to a unit in that layer

# Explanation for Vectorized Implementation

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.5.jpg width="500">

* Why it works: When we multiply the vectors W and X, we get the values necessary to compute the next layer

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.5.2.jpg width="500">

* Quick recap: Code to use and some calculations along with their results
* Andrew points out that there is a kind of pattern to be seen when calculating layer outputs

# Activation functions

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.6.jpg width="500">

* You can use different activations functions in each layer (if you want)
* Some criteria for choosing an activation function:
    * Output is binary? Sigmoid (keeps output between 0 and 1)
    * Some people default to ReLU, can also use tanh
    * Leaky ReLU / ReLU can help to avoid 0-valued derivatives (which in turn leads to vanishing gradients)
        * In sigmoid, derivatives become close to 0 when an output is close to 0 or 1, which leads to slow training
        * With ReLU, our slopes are sharper (i.e. not flattened as hard), and therefore allows us to converge faster

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.6.2.jpg width="500">

* How some activation functions look
* tanh is strictly superior to sigmoid, can default to ReLU if unsure

# Why do you need non-linear activation functions

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.7.jpg width="500">

* Using only linear functions in your network = Using one giant linear function
* Linear models are limited in what they can compute
    * The greater the complexity of the problem, the more likely it is for a linear model to be insufficient

# Derivatives of activation functions

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.8.jpg width="500">

* Derivative of sigmoid of z: g'(z) = a * (1 - a)
    * Where a = g(z) = 1 / (1 + e<sup>-z</sup>)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.8.2.jpg width="500">

* Derivative of tanh of z: g'(z) = 1 - a<sup>2</sup>
    * Where a = g(z) = tanh(z)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.8.3.jpg width="500">

* Derivative of ReLU / Leaky ReLU of z: 0 or 0.01 if z < 0 and 1 if z >= 0 (respectively)
    * Where ReLU / Leaky ReLU is max(0,z) and max(0.1z,z) (respectively)

# Gradient descent for Neural Networks

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.9.jpg width="500">

* n vectors denote the size of parameters for a particular layer
    * example: n<sup>[1]</sup> = vector of parameters for layer 1
* Basic recap:
    1. Propagate our inputs and parameters through the network to get y hat
    2. Calculate loss function over all examples (How wrong are our predictions)
    3. Apply gradient descent, update parameters, propagate again
    4. Repeat until convergence (or some established criterion)

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.9.2.jpg width="500">

* Backpropagation is finding the derivatives at each node
    * Later nodes (i.e. derivatives closer to the input layer) will require applying the chain rule onto earlier derivatives
    * Repeat until you can find the derivatives of the loss (output) w.r.t. our parameters (w and b)
    * keepdims ensures we don't get weird matrices (size (n,) for example) and saves us from potential calculation errors later on

# Backpropagation intuition

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.10.jpg width="500">

* We first start with GD for logistic regression
* Everything happens in a single step -> easier to calculate
    * Calculating GD in a neural network? A bit different...

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.10.2.jpg width="500">

* We are going BACKwards (BACKprop), starting from loss until dA<sup>[2]</sup> to dZ<sup>[2]</sup> to dW<sup>[2]</sup> and dB<sup>[2]</sup>
    * Repeat until we can reach parameters of layer 1
* Andrew skips calculating dA, goes straight to dZ<sup>[2]</sup>, dW<sup>[2]</sup>, dB<sup>[2]</sup>
* Dimension checking can help to ensure backprop is done correctly
    * Dimensions of a parameter vector and its derivative are equal

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.10.3.jpg width="500">

* Backprop, similar to forward propagation, can be vectorized
    * Since we are vectorizing it, the calculations are done over entire inputs (hence the addition of 1/m)

# Random Initialization

<img src=https://github.com/lmoham/deep-learning-specialization/blob/main/1.%20Neural%20Networks%20and%20Deep%20Learning/images/week3/1.11.jpg width="500">

* If weights are the same for two nodes, then you get the same calculations across two nodes
    * This means the extra node is completely useless, since its not helping the network learn anything new
* Therefore we initialize the weights randomly by a factor of 0.01
    * Small is better because if the values are too big, then if we use a sigmoid function, training becomes slow (near-zero gradient)
    * Bias for whatever reason doesn't suffer from the same problem, and can be initialized at zero
